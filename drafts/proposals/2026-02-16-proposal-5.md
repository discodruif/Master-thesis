# Thesis Proposal 5 (Tilburg MSc Finance)

## Title Page
**Title:** Structure‑Informed Neural Networks for American Option Pricing: Transfer Learning, Data Scarcity, and Out‑of‑Sample Robustness  
**Date:** 2026-02-16  
**Name:** Maurits van Eck  
**ANR:** 2062644  
**Program:** MSc Finance, Tilburg University  

---

## Part 1 — Research Question (~200 words)
American options embed an early‑exercise feature that complicates pricing and hedging. Numerical methods (trees, finite differences, Longstaff–Schwartz) can be accurate but computationally heavy in large‑scale applications. Machine learning promises fast approximation once trained, but ML pricing models often require large labeled datasets and may generalize poorly across regimes. Recent work suggests that *structure‑informed* neural networks (embedding jump‑diffusion or PDE structure) and transfer learning can reduce sample complexity and improve robustness.

**RQ:** *Do structure‑informed neural networks with transfer learning price American equity index options more accurately and robustly than generic ML models and standard numerical benchmarks when training data is limited, and does improved pricing translate into improved hedging performance?*

This is economically important because accurate American pricing affects risk management and strategy evaluation in markets where early exercise is relevant (e.g., equity options with dividends, American-style equity options; and for certain products). Even for European-settled index options, the thesis provides a clear methodological contribution: how to combine financial structure with ML to achieve fast, robust pricers and measurable economic gains. The empirical component will use OptionMetrics IvyDB US via WRDS (access verified) and will emphasize out‑of‑sample tests and regime robustness, with clear computational benchmarks.

---

## Part 2 — Literature Review (MAX 1 page)
Black‑Scholes (1973) provides the baseline for European options but does not directly resolve early exercise. For American options, classical approaches include binomial/trinomial lattices, finite differences, and simulation-based methods such as Longstaff and Schwartz (2001) for least‑squares Monte Carlo (LSM). These methods are well understood but can be computationally intensive for repeated valuation across large panels.

Recent ML research explores neural approximators for option values and exercise boundaries. A directly relevant paper from today’s literature review is *Jump Diffusion‑Informed Neural Networks with Transfer Learning for Accurate American Option Pricing under Data Scarcity* (Sun, Huang, Yang, & Zhang, 2024), which proposes injecting jump‑diffusion structure and using transfer learning to improve accuracy when labeled data are scarce.

More broadly, physics‑informed approaches and economic constraints are increasingly used to improve ML generalization in derivatives contexts, with the implied goal of reducing overfitting and ensuring economically plausible outputs. In top finance outlets (JF/JFE/RFS/JFQA/Review of Finance), the key methodological standard is that empirical improvements must be documented out‑of‑sample and connected to economic outcomes (e.g., hedging error net of costs), not only to statistical fit.

This thesis contributes by designing an empirical pipeline that (i) creates training labels via established numerical methods, (ii) evaluates structure‑aware vs generic ML under *controlled data scarcity*, and (iii) tests whether pricing improvements matter for hedging and risk metrics.

---

## Part 3 — Research Plan (~500 words; include regression equations)
### 3.1 Data generating process for training labels
Because true “fundamental” American prices are unobserved, I will construct labels using a trusted numerical benchmark (e.g., LSM with sufficient paths, or finite differences where feasible). The ML model then learns a mapping:
\[
V = f(S, K, r, q, \tau, \sigma, \text{state})
\]
where \(q\) is dividend yield (or proxy), and “state” may include additional factors such as skew/term structure.

### 3.2 Models (and what “structure” means here)
1. **Numerical benchmark:** LSM / tree / finite difference (accuracy benchmark; slower). This is the ground truth generator for training labels and for evaluation.
2. **Generic ML pricer:** MLP or gradient boosting trained on \((S,K,r,q,\tau,\sigma,\ldots)\) to predict the American price directly.
3. **Structure‑informed NN:** incorporate financial structure in at least two ways:
   - **Model-informed inputs:** include jump and stochastic-volatility proxies (e.g., IV skew/term-structure factors) so the network conditions on economically relevant state variables.
   - **Constraint/penalty terms:** enforce monotonicity in \(S\) and convexity in \(K\) where applicable, and penalize violations of lower/upper bounds (e.g., \(V^{Am}\ge V^{Eu}\), \(V^{Am}\ge (S-K)^+\) for calls without dividends).

4. **Transfer learning:** pretrain on a large synthetic dataset generated from broad parameter grids (e.g., jump‑diffusion parameters and volatility regimes), then fine‑tune on market‑relevant regions calibrated from IvyDB. The hypothesis (as in Sun et al., 2024) is that transfer learning reduces sample complexity and improves generalization when real-market labels are scarce.

**Optional exercise-boundary modeling.** As an extension, I will test a two‑network approach: one network predicts the continuation value and another predicts the early‑exercise boundary. This can improve extrapolation because the boundary has more stable structure than raw prices across strikes and regimes.

**Computational metric.** Besides pricing accuracy, I will report speed (prices per second) because “fast approximation” is a key advantage of ML pricers relative to LSM.

### 3.3 Data scarcity experiment
I will run controlled experiments where the number of labeled training samples is restricted (e.g., 10%, 25%, 50% of the full labeled set). Compare out‑of‑sample pricing RMSE and worst‑case errors across models.

### 3.4 Concrete regression equations
**(A) Pricing error decomposition:**
\[
|PE_{i,t}| = \alpha + \beta_1 \text{TTM}_{i,t} + \beta_2 \text{Moneyness}_{i,t} + \beta_3 \text{VIX}_t + \beta_4 \text{BidAsk}_{i,t} + \sum_m \gamma_m \mathbb{1}\{\text{Model}=m\} + \varepsilon_{i,t}
\]
This identifies where each model performs well/poorly and whether structure-informed models reduce errors in stressed regimes.

**(B) Early exercise premium test:** define early exercise premium \(EEP_{i,t} = V^{Am}_{i,t} - V^{Eu}_{i,t}\). Test whether model-implied premiums align with economic determinants:
\[
EEP_{i,t} = a + b_1 \text{DividendProxy}_{t} + b_2 r_t + b_3 \text{TTM}_{i,t} + b_4 \text{Moneyness}_{i,t} + u_{i,t}
\]
A structure-informed model should produce economically sensible sensitivities.

**(C) Hedging implication:** compute delta from each model and evaluate discrete hedging loss:
\[
\Pi_{i} = V_{i,t_0} - V_{i,t_0+H} + \sum_{t=t_0}^{t_0+H-1} h_t (S_{t+1}-S_t) - \lambda \sum |\Delta h_t|S_t
\]
Compare loss distributions and test whether pricing improvements translate into lower net hedging losses.

### 3.5 Robustness and identification
- **Alternative label generators:** LSM vs tree/finite-difference to ensure results are not artifacts of the labeling method. I will also test sensitivity to LSM design (number of paths, basis functions).
- **Alternative objectives:** RMSE vs MAE vs tail metrics (P95 pricing error, CVaR of error). Tail metrics matter because mispricing in the wings can drive large risk.
- **Regime robustness:** evaluate separately in low vs high volatility regimes and around large market moves, where early exercise incentives and nonlinearities can change.
- **Cross-sectional robustness:** compare performance across moneyness and maturity buckets; emphasize whether structure-informed models reduce extrapolation errors for deep ITM/OTM options.
- **Out-of-sample and leakage control:** use time-based splits (train on earlier periods, test on later periods) and ensure that features do not leak future information (e.g., using only information available at time t).

A key identification goal is to isolate the incremental value of “structure.” Therefore I will keep architectures comparable (same capacity where possible) and attribute improvements to added constraints/transfer learning rather than to simply larger models overall.

---

## Part 4 — Data Sources (~400 words; include descriptive statistics table; confirm access)
**Primary data:** **OptionMetrics IvyDB US via WRDS (access verified).** I will use option quotes and implied volatilities for liquid underlyings. Because SPX index options are European-style, the core American-option analysis will focus on **single‑stock equity options** in IvyDB (American style), while SPX options can be used for auxiliary comparisons (European pricing/hedging baselines). This design ensures that the early‑exercise feature is empirically relevant and that the dataset matches the research question.

**Supplementary data:** risk‑free rates, dividend yields or dividend proxies, and volatility indices (VIX). When dividend data are needed, I will use available WRDS sources (e.g., CRSP distributions) or implied dividend approaches.

**Training label construction:** labels derived from numerical pricing methods applied to calibrated parameters. Concretely, I will calibrate a simple underlying dynamics proxy (e.g., local volatility / implied-vol-based mapping, and/or jump-diffusion parameter grids anchored to observed IV level and skew) and then price American options under those parameters using LSM or finite differences. The thesis will carefully document the labeling procedure, including number of Monte Carlo paths, basis functions in LSM, and convergence diagnostics, and will verify that label noise is small relative to market bid–ask spreads.

### Descriptive statistics (template)

| Variable | Mean | Std | P5 | P50 | P95 | N |
|---|---:|---:|---:|---:|---:|---:|
| Option mid price |  |  |  |  |  |  |
| Time-to-maturity (days) |  |  |  |  |  |  |
| Log-moneyness |  |  |  |  |  |  |
| Implied volatility |  |  |  |  |  |  |
| Bid-ask spread |  |  |  |  |  |  |
| Estimated early exercise premium |  |  |  |  |  |  |

Statistics will be reported overall and in moneyness/TTM buckets.

---

## Part 5 — References (APA)
Black, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. *Journal of Political Economy, 81*(3), 637–654.

Longstaff, F. A., & Schwartz, E. S. (2001). Valuing American options by simulation: A simple least-squares approach. *Review of Financial Studies, 14*(1), 113–147.

Sun, Q., Huang, H., Yang, X., & Zhang, Y. (2024). Jump diffusion-informed neural networks with transfer learning for accurate American option pricing under data scarcity. *Working paper* (Semantic Scholar).
